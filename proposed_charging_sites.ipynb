{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b42a99b",
   "metadata": {},
   "source": [
    "### Imports & basic config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "919a71ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core\n",
    "import re, numpy as np, pandas as pd\n",
    "from math import sin, cos, radians, asin, sqrt\n",
    "\n",
    "# Modeling\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Early stopping (newer XGBoost). Safe to ignore if not available.\n",
    "try:\n",
    "    from xgboost.callback import EarlyStopping as XGB_EarlyStopping\n",
    "except Exception:\n",
    "    XGB_EarlyStopping = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cc503a",
   "metadata": {},
   "source": [
    "### Data Injest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1134df52",
   "metadata": {},
   "outputs": [],
   "source": [
    "ev = pd.read_csv(\"/home/britton/Projects/Predictive_Analysis_1/data/Electric_Vehicle_Population_Data.csv\")\n",
    "cs = pd.read_csv(\"/home/britton/Projects/Predictive_Analysis_1/data/charging_stations.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3b9128",
   "metadata": {},
   "source": [
    "### H3 helpers (v3/v4 compatible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ce3fbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version-agnostic wrappers over h3 (v4 preferred, v3 fallback)\n",
    "try:\n",
    "    import h3\n",
    "    def to_cell(lat, lon, res): return h3.latlng_to_cell(lat, lon, res)\n",
    "    def cell_center(h): return h3.cell_to_latlng(h)        # (lat, lon)\n",
    "    def parent(h, res): return h3.cell_to_parent(h, res)\n",
    "    def neighbors(h, k): return h3.grid_disk(h, k)         # includes center\n",
    "except Exception:\n",
    "    from h3 import h3 as h3v3\n",
    "    def to_cell(lat, lon, res): return h3v3.geo_to_h3(lat, lon, res)\n",
    "    def cell_center(h): return h3v3.h3_to_geo(h)           # (lat, lon)\n",
    "    def parent(h, res): return h3v3.h3_to_parent(h, res)\n",
    "    def neighbors(h, k): return h3v3.k_ring(h, k)          # includes center\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d69a29",
   "metadata": {},
   "source": [
    "### Helpers: WKT parser + Haversine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b75548ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_point_wkt(s):\n",
    "    \"\"\"\n",
    "    Parse 'POINT(lon lat)' from a WKT-like string.\n",
    "    Returns (lat, lon); returns (nan, nan) on failure/NaN.\n",
    "    \"\"\"\n",
    "    if pd.isna(s): return np.nan, np.nan\n",
    "    m = re.search(r\"POINT\\s*\\(\\s*([-\\d\\.]+)\\s+([-\\d\\.]+)\\s*\\)\", str(s))\n",
    "    if not m: return np.nan, np.nan\n",
    "    lon, lat = float(m.group(1)), float(m.group(2))\n",
    "    return lat, lon\n",
    "\n",
    "def hav_km(lat1, lon1, lat2, lon2):\n",
    "    R=6371.0\n",
    "    dlat, dlon = radians(lat2-lat1), radians(lon2-lon1)\n",
    "    a = sin(dlat/2)**2 + cos(radians(lat1))*cos(radians(lat2))*sin(dlon/2)**2\n",
    "    return 2*R*asin(sqrt(a))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f609db",
   "metadata": {},
   "source": [
    "### Input DataFrames (ev, cs) + basic cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b25114b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expect ev, cs already in memory.\n",
    "\n",
    "ev = ev.copy()\n",
    "\n",
    "# Ensure lat/lon exist (either via WKT or Latitude/Longitude)\n",
    "if \"lat\" not in ev.columns or \"lon\" not in ev.columns:\n",
    "    if \"Vehicle Location\" in ev.columns:\n",
    "        ev[\"lat\"], ev[\"lon\"] = zip(*ev[\"Vehicle Location\"].map(parse_point_wkt))\n",
    "    else:\n",
    "        ev = ev.rename(columns={\"Latitude\":\"lat\",\"Longitude\":\"lon\"})\n",
    "\n",
    "# Filter to WA if available, coerce coords, drop bad rows\n",
    "if \"State\" in ev.columns:\n",
    "    ev = ev[ev[\"State\"].astype(str).str.upper().eq(\"WA\")]\n",
    "ev[\"lat\"] = pd.to_numeric(ev[\"lat\"], errors=\"coerce\")\n",
    "ev[\"lon\"] = pd.to_numeric(ev[\"lon\"], errors=\"coerce\")\n",
    "ev = ev.dropna(subset=[\"lat\",\"lon\"]).copy()\n",
    "\n",
    "# Stations table (cs -> st), filter and normalize schema\n",
    "st = cs.copy()\n",
    "if \"Fuel Type Code\" in st.columns:\n",
    "    st = st[st[\"Fuel Type Code\"].astype(str).str.upper().eq(\"ELEC\")]\n",
    "if \"State\" in st.columns:\n",
    "    st = st[st[\"State\"].astype(str).str.upper().eq(\"WA\")]\n",
    "st = st.rename(columns={\n",
    "    \"Latitude\":\"lat\",\"Longitude\":\"lon\",\n",
    "    \"EV DC Fast Count\":\"dcfc_count\",\n",
    "    \"EV Level2 EVSE Num\":\"l2_count\"\n",
    "})\n",
    "st[\"lat\"] = pd.to_numeric(st[\"lat\"], errors=\"coerce\")\n",
    "st[\"lon\"] = pd.to_numeric(st[\"lon\"], errors=\"coerce\")\n",
    "for c in [\"dcfc_count\",\"l2_count\"]:\n",
    "    if c in st.columns:\n",
    "        st[c] = pd.to_numeric(st[c], errors=\"coerce\").fillna(0).astype(int)\n",
    "    else:\n",
    "        st[c] = 0\n",
    "st = st.dropna(subset=[\"lat\",\"lon\"]).copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f1c648",
   "metadata": {},
   "source": [
    "### H3 assignment + per-hex aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d76cba79",
   "metadata": {},
   "outputs": [],
   "source": [
    "RES = 8  # neighborhood-ish scale\n",
    "\n",
    "# Assign EVs & stations to hexes\n",
    "ev[\"h3\"] = [to_cell(r.lat, r.lon, RES) for r in ev.itertuples()]\n",
    "st[\"h3\"] = [to_cell(r.lat, r.lon, RES) for r in st.itertuples()]\n",
    "\n",
    "# Per-hex aggregates\n",
    "ev_cnt = ev.groupby(\"h3\").size().rename(\"ev_cnt\")\n",
    "st_cnt = st.groupby(\"h3\").size().rename(\"station_cnt\")\n",
    "dcfc_cnt = st.groupby(\"h3\")[\"dcfc_count\"].sum().rename(\"dcfc_ports\")\n",
    "l2_cnt   = st.groupby(\"h3\")[\"l2_count\"].sum().rename(\"l2_ports\")\n",
    "\n",
    "# Join and label (1 if station exists in hex)\n",
    "df = pd.concat([ev_cnt, st_cnt, dcfc_cnt, l2_cnt], axis=1).fillna(0)\n",
    "df[\"label\"] = (df[\"station_cnt\"] > 0).astype(int)\n",
    "\n",
    "# Hex centers for distance + map\n",
    "centers = pd.DataFrame({h: cell_center(h) for h in df.index}).T.rename(columns={0:\"lat_c\",1:\"lon_c\"})\n",
    "df = df.join(centers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc25d96",
   "metadata": {},
   "source": [
    "### Neighborhood (k=2) features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff49da60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ksum(series_map, h, k=2):\n",
    "    ns = set(neighbors(h, k))\n",
    "    ns.discard(h)                 # drop self\n",
    "    return sum(series_map.get(n, 0) for n in ns)\n",
    "\n",
    "\n",
    "# Precompute dicts for fast lookup\n",
    "ev_map, st_map, dc_map, l2_map = ev_cnt.to_dict(), st_cnt.to_dict(), dcfc_cnt.to_dict(), l2_cnt.to_dict()\n",
    "\n",
    "# k=2 totals (context)\n",
    "df[\"ev_cnt_k2\"]     = [ksum(ev_map, h, 2) for h in df.index]\n",
    "df[\"st_cnt_k2\"]     = [ksum(st_map, h, 2) for h in df.index]\n",
    "df[\"dcfc_ports_k2\"] = [ksum(dc_map, h, 2) for h in df.index]\n",
    "df[\"l2_ports_k2\"]   = [ksum(l2_map, h, 2) for h in df.index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eb210c",
   "metadata": {},
   "source": [
    "### Distance-to-nearest-station feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28a31b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build array of station hex centers\n",
    "station_hexes = st[\"h3\"].unique().tolist()\n",
    "station_centers = np.array([cell_center(h) for h in station_hexes])\n",
    "\n",
    "def nearest_station_km(lat, lon):\n",
    "    \"\"\"Brute-force nearest station distance (km).\"\"\"\n",
    "    if station_centers.size == 0: return np.nan\n",
    "    return float(np.min([hav_km(lat, lon, slat, slon) for slat, slon in station_centers]))\n",
    "\n",
    "df[\"dist_to_station_km\"] = [nearest_station_km(r.lat_c, r.lon_c) for r in df.itertuples()]\n",
    "df[\"dist_to_station_km\"] = df[\"dist_to_station_km\"].replace([np.inf,-np.inf], np.nan)\n",
    "df[\"dist_to_station_km\"] = df[\"dist_to_station_km\"].fillna(df[\"dist_to_station_km\"].max() or 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebef2c28",
   "metadata": {},
   "source": [
    "### Spatial CV training & metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b854ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spatial CV AUROC: 0.999 ± 0.001\n",
      "Spatial CV AUPRC: 1.000 ± 0.000\n"
     ]
    }
   ],
   "source": [
    "FEATURES = [\"ev_cnt\",\"ev_cnt_k2\",\"st_cnt_k2\",\"dcfc_ports_k2\",\"l2_ports_k2\",\"dist_to_station_km\"]\n",
    "X, y = df[FEATURES].copy(), df[\"label\"].astype(int)\n",
    "\n",
    "# Group by parent hexes at a coarser res to reduce leakage across folds\n",
    "GROUP_RES = 5\n",
    "groups = [parent(h, GROUP_RES) for h in df.index]\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "def fit_xgb(model, Xtr, ytr, Xva, yva):\n",
    "    \"\"\"Try modern callbacks -> early_stopping_rounds -> no ES.\"\"\"\n",
    "    if XGB_EarlyStopping is not None:\n",
    "        try:\n",
    "            return model.fit(\n",
    "                Xtr, ytr, eval_set=[(Xva, yva)], verbose=False,\n",
    "                callbacks=[XGB_EarlyStopping(rounds=50, save_best=True)]\n",
    "            )\n",
    "        except TypeError:\n",
    "            pass\n",
    "    try:\n",
    "        return model.fit(\n",
    "            Xtr, ytr, eval_set=[(Xva, yva)], verbose=False,\n",
    "            early_stopping_rounds=50\n",
    "        )\n",
    "    except TypeError:\n",
    "        return model.fit(Xtr, ytr, eval_set=[(Xva, yva)], verbose=False)\n",
    "\n",
    "aucs, auprcs = [], []\n",
    "for tr, va in gkf.split(X, y, groups):\n",
    "    Xtr, Xva = X.iloc[tr], X.iloc[va]\n",
    "    ytr, yva = y.iloc[tr], y.iloc[va]\n",
    "    pos = max(ytr.mean(), 1e-8)\n",
    "    spw = (1 - pos) / pos\n",
    "\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=600, learning_rate=0.05, max_depth=6,\n",
    "        subsample=0.8, colsample_bytree=0.8, reg_lambda=2.0,\n",
    "        objective=\"binary:logistic\", eval_metric=\"logloss\",\n",
    "        tree_method=\"hist\", random_state=42, scale_pos_weight=spw\n",
    "    )\n",
    "    fit_xgb(model, Xtr, ytr, Xva, yva)\n",
    "    p = model.predict_proba(Xva)[:, 1]  # <-- P(station_present) on validation fold\n",
    "    aucs.append(roc_auc_score(yva, p))\n",
    "    auprcs.append(average_precision_score(yva, p))\n",
    "\n",
    "print(f\"Spatial CV AUROC: {np.mean(aucs):.3f} ± {np.std(aucs):.3f}\")\n",
    "print(f\"Spatial CV AUPRC: {np.mean(auprcs):.3f} ± {np.std(auprcs):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ce2283",
   "metadata": {},
   "source": [
    "### Final fit on all data + per-hex score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "67a8e89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = max(y.mean(), 1e-8)\n",
    "spw = (1 - pos) / pos\n",
    "\n",
    "final_model = XGBClassifier(\n",
    "    n_estimators=600, learning_rate=0.05, max_depth=6,\n",
    "    subsample=0.8, colsample_bytree=0.8, reg_lambda=2.0,\n",
    "    objective=\"binary:logistic\", eval_metric=\"logloss\",\n",
    "    tree_method=\"hist\", random_state=42, scale_pos_weight=spw\n",
    ")\n",
    "final_model.fit(X, y, verbose=False)\n",
    "\n",
    "# <-- Prediction for every hex: P(station_present)\n",
    "df[\"score\"] = final_model.predict_proba(X)[:, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66ef21e",
   "metadata": {},
   "source": [
    "### Candidate selection & CSV export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "32b5fa3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote charging_site_candidates.csv\n"
     ]
    }
   ],
   "source": [
    "# Keep consistent hex id column on export\n",
    "df = df.copy()\n",
    "df.index.name = \"h3\"\n",
    "\n",
    "# Policy: no current station AND >= 2 km from nearest station\n",
    "candidates = df[df[\"label\"] == 0].copy()\n",
    "candidates = candidates[candidates[\"dist_to_station_km\"] >= 2.0]\n",
    "\n",
    "out = candidates.sort_values(\"score\", ascending=False).reset_index()\n",
    "if \"h3\" not in out.columns and \"index\" in out.columns:\n",
    "    out = out.rename(columns={\"index\": \"h3\"})\n",
    "out = out.rename(columns={\"h3\": \"h3_cell\"})\n",
    "\n",
    "cols = [\n",
    "    \"h3_cell\",\"lat_c\",\"lon_c\",\n",
    "    \"ev_cnt\",\"ev_cnt_k2\",\n",
    "    \"st_cnt_k2\",\"dcfc_ports_k2\",\"l2_ports_k2\",\n",
    "    \"dist_to_station_km\",\"score\"\n",
    "]\n",
    "\n",
    "missing = [c for c in cols if c not in out.columns]\n",
    "if missing:\n",
    "    raise KeyError(f\"Missing columns in export: {missing}. Present: {list(out.columns)}\")\n",
    "\n",
    "out[cols].to_csv(\"/home/britton/Projects/Predictive_Analysis_1/data/charging_site_candidates.csv\", index=False)\n",
    "print(\"Wrote charging_site_candidates.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223a8b3e",
   "metadata": {},
   "source": [
    "### (Optional) Feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ad59cbb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xgb_gain</th>\n",
       "      <th>xgb_cover</th>\n",
       "      <th>xgb_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dist_to_station_km</th>\n",
       "      <td>53.092361</td>\n",
       "      <td>32.850357</td>\n",
       "      <td>136.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ev_cnt_k2</th>\n",
       "      <td>14.531989</td>\n",
       "      <td>54.808544</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ev_cnt</th>\n",
       "      <td>9.291797</td>\n",
       "      <td>9.821816</td>\n",
       "      <td>241.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>l2_ports_k2</th>\n",
       "      <td>0.907876</td>\n",
       "      <td>9.897997</td>\n",
       "      <td>137.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>st_cnt_k2</th>\n",
       "      <td>0.605632</td>\n",
       "      <td>11.132406</td>\n",
       "      <td>106.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dcfc_ports_k2</th>\n",
       "      <td>0.190432</td>\n",
       "      <td>6.937555</td>\n",
       "      <td>126.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     xgb_gain  xgb_cover  xgb_weight\n",
       "dist_to_station_km  53.092361  32.850357       136.0\n",
       "ev_cnt_k2           14.531989  54.808544        19.0\n",
       "ev_cnt               9.291797   9.821816       241.0\n",
       "l2_ports_k2          0.907876   9.897997       137.0\n",
       "st_cnt_k2            0.605632  11.132406       106.0\n",
       "dcfc_ports_k2        0.190432   6.937555       126.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_names = list(X.columns)\n",
    "booster = final_model.get_booster()\n",
    "name_map = {f\"f{i}\": feat_names[i] for i in range(len(feat_names))}\n",
    "\n",
    "def imp_df(importance_type=\"gain\"):\n",
    "    raw = booster.get_score(importance_type=importance_type)\n",
    "    if not raw:\n",
    "        return pd.Series(0.0, index=feat_names, name=f\"xgb_{importance_type}\")\n",
    "    # If keys look like 'f0', map them; if keys are real names, use as-is\n",
    "    if set(raw.keys()) <= set(name_map.keys()):\n",
    "        s = pd.Series({name_map[k]: v for k, v in raw.items()})\n",
    "    else:\n",
    "        s = pd.Series(raw)\n",
    "    s = s.reindex(feat_names).fillna(0.0).sort_values(ascending=False)\n",
    "    s.name = f\"xgb_{importance_type}\"\n",
    "    return s\n",
    "\n",
    "gain_imp   = imp_df(\"gain\")\n",
    "cover_imp  = imp_df(\"cover\")\n",
    "weight_imp = imp_df(\"weight\")\n",
    "pd.concat([gain_imp, cover_imp, weight_imp], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6818aa",
   "metadata": {},
   "source": [
    "### Folium map (load CSV and render)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7572d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote wa_ev_charging_candidates_map.html\n"
     ]
    }
   ],
   "source": [
    "import folium\n",
    "from folium import LayerControl\n",
    "from branca.colormap import linear\n",
    "\n",
    "CAND_PATH = \"/home/britton/Projects/Predictive_Analysis_1/data/charging_site_candidates.csv\"\n",
    "TOP_N = 300\n",
    "\n",
    "cand = pd.read_csv(CAND_PATH)\n",
    "if \"h3_cell\" not in cand.columns:\n",
    "    if \"h3\" in cand.columns:\n",
    "        cand = cand.rename(columns={\"h3\": \"h3_cell\"})\n",
    "    elif \"index\" in cand.columns:\n",
    "        cand = cand.rename(columns={\"index\": \"h3_cell\"})\n",
    "\n",
    "required_cols = {\"h3_cell\", \"lat_c\", \"lon_c\", \"score\"}\n",
    "missing = required_cols - set(cand.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Candidates CSV missing required columns: {missing}\")\n",
    "\n",
    "cand = cand.dropna(subset=[\"h3_cell\", \"lat_c\", \"lon_c\", \"score\"]).copy()\n",
    "smin, smax = cand[\"score\"].min(), cand[\"score\"].max()\n",
    "cand[\"score_norm\"] = (cand[\"score\"] - smin) / (smax - smin + 1e-9)\n",
    "\n",
    "# H3 boundary helper for map\n",
    "try:\n",
    "    import h3  # v4+\n",
    "    def hex_boundary(h): return h3.cell_to_boundary(h)          # [(lat, lon), ...]\n",
    "except Exception:\n",
    "    from h3 import h3 as h3v3\n",
    "    def hex_boundary(h): return h3v3.h3_to_geo_boundary(h)      # [(lat, lon), ...]\n",
    "\n",
    "# Station points (from in-memory cs)\n",
    "st_map = cs.copy().rename(columns={\"Latitude\":\"lat\",\"Longitude\":\"lon\"}).dropna(subset=[\"lat\",\"lon\"]).copy()\n",
    "\n",
    "# Base map\n",
    "m = folium.Map(location=[47.5, -120.5], zoom_start=6, tiles=\"cartodbpositron\")\n",
    "cmap = linear.YlOrRd_09.scale(0, 1); cmap.caption = \"Candidate score (higher = more promising)\"; cmap.add_to(m)\n",
    "\n",
    "# Top-N polygons with tooltips\n",
    "top_layer = folium.FeatureGroup(name=f\"Top {TOP_N} candidates\", show=True)\n",
    "for r in cand.nlargest(TOP_N, \"score\").itertuples():\n",
    "    try:\n",
    "        poly = hex_boundary(r.h3_cell)\n",
    "        folium.Polygon(\n",
    "            locations=poly,\n",
    "            tooltip=(f\"<b>Score:</b> {r.score:.3f}<br>\"\n",
    "                     f\"<b>EV in hex:</b> {int(getattr(r, 'ev_cnt', 0))} | \"\n",
    "                     f\"<b>EV k2:</b> {int(getattr(r, 'ev_cnt_k2', 0))}<br>\"\n",
    "                     f\"<b>Stations k2:</b> {int(getattr(r, 'st_cnt_k2', 0))} | \"\n",
    "                     f\"<b>DCFC k2:</b> {int(getattr(r, 'dcfc_ports_k2', 0))} | \"\n",
    "                     f\"<b>L2 k2:</b> {int(getattr(r, 'l2_ports_k2', 0))}<br>\"\n",
    "                     f\"<b>Dist to station:</b> {float(getattr(r, 'dist_to_station_km', np.nan)):.1f} km\"),\n",
    "            weight=1, opacity=0.8, fill_opacity=0.55, color=\"#333333\",\n",
    "            fill_color=cmap(r.score_norm)\n",
    "        ).add_to(top_layer)\n",
    "    except Exception:\n",
    "        continue\n",
    "top_layer.add_to(m)\n",
    "\n",
    "# All candidates (heavier)\n",
    "all_layer = folium.FeatureGroup(name=\"All candidates\", show=False)\n",
    "for r in cand.itertuples():\n",
    "    try:\n",
    "        poly = hex_boundary(r.h3_cell)\n",
    "        folium.Polygon(\n",
    "            locations=poly, weight=0.3, opacity=0.5, fill_opacity=0.35,\n",
    "            color=\"#555555\", fill_color=cmap(r.score_norm)\n",
    "        ).add_to(all_layer)\n",
    "    except Exception:\n",
    "        continue\n",
    "all_layer.add_to(m)\n",
    "\n",
    "# Existing stations\n",
    "st_layer = folium.FeatureGroup(name=\"Existing stations\", show=True)\n",
    "for r in st_map.itertuples():\n",
    "    try:\n",
    "        folium.CircleMarker(\n",
    "            location=(float(r.lat), float(r.lon)),\n",
    "            radius=2.5, fill=True, fill_opacity=0.8, opacity=0.8\n",
    "        ).add_to(st_layer)\n",
    "    except Exception:\n",
    "        continue\n",
    "st_layer.add_to(m)\n",
    "\n",
    "LayerControl(collapsed=False).add_to(m)\n",
    "m.save(\"/home/britton/Projects/Predictive_Analysis_1/data/wa_ev_charging_candidates_map.html\")\n",
    "print(\"Wrote wa_ev_charging_candidates_map.html\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Predictive_Analysis_1)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
